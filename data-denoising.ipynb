{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9088747,"sourceType":"datasetVersion","datasetId":5484251}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install obspy\n!pip install numpy pandas matplotlib\n!pip install scipy\n!pip install keras\n!pip install tensorflow\n!pip install wurlitzer\n\nimport os\nimport obspy.clients.fdsn\nimport numpy as np\nimport pandas as pd\nimport obspy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\nfrom datetime import datetime\nimport scipy\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport pylab as plt\nimport warnings\nimport os\nimport h5py\nimport matplotlib as mpl\n%matplotlib inline\n\nimport sklearn\nfrom sklearn import linear_model\n\nimport pandas as pd\nimport seaborn as sns\n\nimport keras\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, LSTM\nfrom keras import losses\nfrom keras.utils import to_categorical\nfrom keras.layers import BatchNormalization\nimport keras.backend as K\n\nfrom __future__ import print_function\n\nfrom collections import defaultdict\nimport pickle\nfrom PIL import Image\n\nfrom six.moves import range\n\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout, LeakyReLU\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import Progbar\nimport numpy as np\n\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import expand_dims\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Reshape\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras.initializers import RandomNormal\nfrom matplotlib import pyplot\n\nnp.random.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T12:13:23.967798Z","iopub.execute_input":"2024-08-20T12:13:23.968701Z","iopub.status.idle":"2024-08-20T12:14:39.543299Z","shell.execute_reply.started":"2024-08-20T12:13:23.968666Z","shell.execute_reply":"2024-08-20T12:14:39.540255Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting obspy\n  Downloading obspy-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from obspy) (1.26.4)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from obspy) (1.11.4)\nRequirement already satisfied: matplotlib>=3.3 in /opt/conda/lib/python3.10/site-packages (from obspy) (3.7.5)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from obspy) (5.2.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from obspy) (69.0.3)\nCollecting sqlalchemy<2 (from obspy)\n  Downloading SQLAlchemy-1.4.53-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from obspy) (5.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from obspy) (2.32.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3->obspy) (2.9.0.post0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<2->obspy) (3.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->obspy) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->obspy) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->obspy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->obspy) (2024.7.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3->obspy) (1.16.0)\nDownloading obspy-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading SQLAlchemy-1.4.53-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (3.4.1)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras) (3.10.0)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras) (0.12.1)\nRequirement already satisfied: ml-dtypes in /opt/conda/lib/python3.10/site-packages (from keras) (0.2.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras) (21.3)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from optree->keras) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras) (3.1.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow)\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.32.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.1.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\nDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.15.0\nCollecting wurlitzer\n  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\nDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\nInstalling collected packages: wurlitzer\nSuccessfully installed wurlitzer-3.1.1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install wurlitzer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mobspy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfdsn\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'obspy'"],"ename":"ModuleNotFoundError","evalue":"No module named 'obspy'","output_type":"error"}]},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.544115Z","iopub.status.idle":"2024-08-20T12:14:39.544482Z","shell.execute_reply.started":"2024-08-20T12:14:39.544306Z","shell.execute_reply":"2024-08-20T12:14:39.544321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\n\n# Define the folder path\nfolder_path = '/kaggle/input/dataset-noise'\n\n# Load numpy arrays from the specified folder\nW = np.load(os.path.join(folder_path, '/kaggle/input/dataset-noise/IRIS_dataset_waveforms_train(1).npy'))\nY = np.load(os.path.join(folder_path, '/kaggle/input/dataset-noise/IRIS_dataset_labels_train(1).npy'))\n\nprint(W.shape, Y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.545742Z","iopub.status.idle":"2024-08-20T12:14:39.546049Z","shell.execute_reply.started":"2024-08-20T12:14:39.545897Z","shell.execute_reply":"2024-08-20T12:14:39.545910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"W = W.reshape(-1,12000,3,1)\nW.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.547170Z","iopub.status.idle":"2024-08-20T12:14:39.547488Z","shell.execute_reply.started":"2024-08-20T12:14:39.547328Z","shell.execute_reply":"2024-08-20T12:14:39.547341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef add_multi_type_noise(clean_data):\n    noisy_data = clean_data.copy()\n    noise_types = []\n    \n    # Gaussian noise\n    if np.random.random() < 0.5:\n        noise = np.random.normal(0, 0.1, noisy_data.shape)\n        noisy_data += noise\n        noise_types.append('gaussian')\n    \n    # Salt and Pepper noise\n    if np.random.random() < 0.5:\n        salt = np.random.random(noisy_data.shape) < 0.05\n        pepper = np.random.random(noisy_data.shape) < 0.05\n        noisy_data[salt] = 1\n        noisy_data[pepper] = -1\n        noise_types.append('salt_and_pepper')\n    \n    # Speckle noise\n    if np.random.random() < 0.5:\n        noise = np.random.randn(*noisy_data.shape)\n        noisy_data += noisy_data * noise * 0.1\n        noise_types.append('speckle')\n    \n    return noisy_data, noise_types\n\n# Update W and Y\nW_noisy = np.array([add_multi_type_noise(w)[0] for w in W])\n\n# Update Y to be the clean data\nY = W.copy()\n\n# Reshape W_noisy and Y if necessary\nW_noisy = W_noisy.reshape(-1, 3, 12000, 1)\nY = Y.reshape(-1, 3, 12000, 1)\n\n# Convert to float32\nW_noisy = W_noisy.astype('float32')\nY = Y.astype('float32')\n\nprint(\"W_noisy shape:\", W_noisy.shape)\nprint(\"Y shape:\", Y.shape)\n\n# Save W_noisy\nnp.save('/kaggle/working/W_noisy.npy', W_noisy)\n\n# Save Y\nnp.save('/kaggle/working/Y_clean.npy', Y)\n\nprint(\"Data saved successfully using numpy.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.548668Z","iopub.status.idle":"2024-08-20T12:14:39.549005Z","shell.execute_reply.started":"2024-08-20T12:14:39.548836Z","shell.execute_reply":"2024-08-20T12:14:39.548850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.pyplot import plot\nfor i in range(3):\n    plt.plot(W_noisy[23, i, :])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.549968Z","iopub.status.idle":"2024-08-20T12:14:39.550320Z","shell.execute_reply.started":"2024-08-20T12:14:39.550150Z","shell.execute_reply":"2024-08-20T12:14:39.550169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.pyplot import plot\nfor i in range(3):\n    plt.plot(Y[232, i, :])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.551481Z","iopub.status.idle":"2024-08-20T12:14:39.551787Z","shell.execute_reply.started":"2024-08-20T12:14:39.551633Z","shell.execute_reply":"2024-08-20T12:14:39.551646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n# First, split into train+val and test sets\nW_trainval, W_test, Y_trainval, Y_test = train_test_split(W_noisy, Y, test_size=0.2, random_state=42)\n\n# Then split train+val into train and validation sets\nW_train, W_val, Y_train, Y_val = train_test_split(W_trainval, Y_trainval, test_size=0.2, random_state=42)\n\nprint(\"Train set shape:\", W_train.shape)\nprint(\"Validation set shape:\", W_val.shape)\nprint(\"Test set shape:\", W_test.shape)\n\n# Create TensorFlow datasets\nbatch_size = 32  # Adjust this based on your GPU memory\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((W_train, Y_train))\ntrain_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((W_val, Y_val))\nval_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((W_test, Y_test))\ntest_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# Additional preprocessing steps\ndef preprocess(noisy, clean):\n    # Ensure data types are correct\n    noisy = tf.cast(noisy, tf.float32)\n    clean = tf.cast(clean, tf.float32)\n    \n    # Data augmentation (if needed)\n    # For example, you could add random flipping:\n    if tf.random.uniform(()) > 0.5:\n        noisy = tf.reverse(noisy, axis=[1])\n        clean = tf.reverse(clean, axis=[1])\n    \n    return noisy, clean\n\n# Apply preprocessing to the datasets\ntrain_dataset = train_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\ntest_dataset = test_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Check the shapes\nfor noisy_batch, clean_batch in train_dataset.take(1):\n    print(\"Batch shapes:\")\n    print(\"Noisy:\", noisy_batch.shape)\n    print(\"Clean:\", clean_batch.shape)\n\n# Calculate the number of batches\ntrain_batches = tf.data.experimental.cardinality(train_dataset).numpy()\nval_batches = tf.data.experimental.cardinality(val_dataset).numpy()\ntest_batches = tf.data.experimental.cardinality(test_dataset).numpy()\n\nprint(f\"Number of training batches: {train_batches}\")\nprint(f\"Number of validation batches: {val_batches}\")\nprint(f\"Number of test batches: {test_batches}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.553591Z","iopub.status.idle":"2024-08-20T12:14:39.553928Z","shell.execute_reply.started":"2024-08-20T12:14:39.553763Z","shell.execute_reply":"2024-08-20T12:14:39.553778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.pyplot import plot\nfor i in range(3):\n    plt.plot(W_test[13, i, :])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.555143Z","iopub.status.idle":"2024-08-20T12:14:39.555489Z","shell.execute_reply.started":"2024-08-20T12:14:39.555322Z","shell.execute_reply":"2024-08-20T12:14:39.555336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib.pyplot import plot\nfor i in range(3):\n    plt.plot(Y_test[13, i, :])","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.556526Z","iopub.status.idle":"2024-08-20T12:14:39.556846Z","shell.execute_reply.started":"2024-08-20T12:14:39.556687Z","shell.execute_reply":"2024-08-20T12:14:39.556701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_nan_inf(noisy, clean):\n    noisy = tf.where(tf.math.is_finite(noisy), noisy, 0.0)\n    clean = tf.where(tf.math.is_finite(clean), clean, 0.0)\n    return noisy, clean\n\ntrain_dataset = train_dataset.map(remove_nan_inf, num_parallel_calls=tf.data.AUTOTUNE)\nval_dataset = val_dataset.map(remove_nan_inf, num_parallel_calls=tf.data.AUTOTUNE)\ntest_dataset = test_dataset.map(remove_nan_inf, num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.558473Z","iopub.status.idle":"2024-08-20T12:14:39.558800Z","shell.execute_reply.started":"2024-08-20T12:14:39.558640Z","shell.execute_reply":"2024-08-20T12:14:39.558653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''print(\"Train label distribution:\", np.bincount(Y_train.flatten().astype(int)))\nprint(\"Validation label distribution:\", np.bincount(Y_val.flatten().astype(int)))\nprint(\"Test label distribution:\", np.bincount(Y_test.flatten().astype(int)))\n'''","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.559801Z","iopub.status.idle":"2024-08-20T12:14:39.560139Z","shell.execute_reply.started":"2024-08-20T12:14:39.559963Z","shell.execute_reply":"2024-08-20T12:14:39.559977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nfrom tensorflow.keras import mixed_precision\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)'''","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.561098Z","iopub.status.idle":"2024-08-20T12:14:39.561442Z","shell.execute_reply.started":"2024-08-20T12:14:39.561285Z","shell.execute_reply":"2024-08-20T12:14:39.561299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Concatenate, Conv2D, BatchNormalization, Activation, Embedding, Flatten, UpSampling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom collections import defaultdict\nfrom tensorflow.keras.utils import Progbar\nfrom tensorflow.keras.layers import MaxPooling2D\n\nfrom tensorflow.keras import layers, models, Input\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef build_generator(latent_dim=100):\n    # Noisy waveform input\n    waveform_input = tf.keras.layers.Input(shape=(3, 12000, 1), name='waveform_input')\n    x_waveform = tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(waveform_input)\n    \n    # Noise input\n    noise_input = tf.keras.layers.Input(shape=(latent_dim,), name='noise_input')\n    \n    # Process noise input\n    x_noise = layers.Dense(3 * 12000 * 1)(noise_input)\n    x_noise = layers.Reshape((3, 12000, 1))(x_noise)\n    \n    # Add noise to waveform input instead of concatenating\n    x = layers.Add()([x_waveform, x_noise])\n    \n    # Encoder\n    encoder_layers = []\n    for filters in [64, 128, 256, 512]:\n        x = layers.Conv2D(filters, (3, 3), strides=(1, 2), padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n        encoder_layers.append(x)\n    \n    # Bottleneck\n    x = layers.Conv2D(512, (3, 3), strides=(1, 2), padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(alpha=0.2)(x)\n    \n    # Decoder\n    for filters in [512, 256, 128, 64]:\n        x = layers.Conv2DTranspose(filters, (3, 3), strides=(1, 2), padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n        if len(encoder_layers) > 0:  # Skip connection\n            skip_connection = encoder_layers.pop()\n            x = layers.Concatenate()([x, skip_connection])\n    \n    # Output layer\n    x = layers.Conv2DTranspose(filters, (3, 3), strides=(1, 2), padding='same')(x)\n    x = layers.Conv2DTranspose(1, (3, 3), strides=(1, 1), padding='same', activation='tanh')(x)\n    \n    x = tf.cast(x, tf.float32)\n    return tf.keras.Model(inputs=[waveform_input, noise_input], outputs=x)\n\ngenerator = build_generator()\ngenerator.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.562562Z","iopub.status.idle":"2024-08-20T12:14:39.562892Z","shell.execute_reply.started":"2024-08-20T12:14:39.562731Z","shell.execute_reply":"2024-08-20T12:14:39.562744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tensorflow-addons","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.564467Z","iopub.status.idle":"2024-08-20T12:14:39.564808Z","shell.execute_reply.started":"2024-08-20T12:14:39.564625Z","shell.execute_reply":"2024-08-20T12:14:39.564638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow_addons.layers import SpectralNormalization\n\nclass SpectralNormalizationConstraint(tf.keras.constraints.Constraint):\n    def __init__(self):\n        self.iteration = 0\n\n    def __call__(self, w):\n        self.iteration += 1\n        if self.iteration % 50 == 0:  # Update u every 50 iterations\n            self.w_shape = w.shape\n            w = tf.reshape(w, [-1, w.shape[-1]])\n            u = tf.random.normal([w.shape[0], 1])\n            v = tf.random.normal([w.shape[1], 1])\n            for _ in range(10):  # Power iteration\n                v = tf.matmul(tf.transpose(w), u)\n                v = v / tf.norm(v)\n                u = tf.matmul(w, v)\n                u = u / tf.norm(u)\n            sigma = tf.matmul(tf.matmul(tf.transpose(u), w), v)\n            w = w / sigma\n            w = tf.reshape(w, self.w_shape)\n        return w\n\ndef build_discriminator():\n    inputs = tf.keras.layers.Input(shape=(3, 12000, 1))\n    x = tf.keras.layers.Lambda(lambda x: tf.cast(x, tf.float32))(inputs)\n    \n    for filters in [64, 128, 256, 512]:\n        x = layers.Conv2D(filters, (3, 3), strides=(1, 2), padding='same', \n                          kernel_constraint=SpectralNormalizationConstraint())(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.LeakyReLU(alpha=0.2)(x)\n    \n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(1, activation='sigmoid', \n                     kernel_constraint=SpectralNormalizationConstraint())(x)\n    \n    x = tf.cast(x, tf.float32)\n    return tf.keras.Model(inputs=inputs, outputs=x)\n\ndiscriminator = build_discriminator()\ndiscriminator.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.565783Z","iopub.status.idle":"2024-08-20T12:14:39.566176Z","shell.execute_reply.started":"2024-08-20T12:14:39.565949Z","shell.execute_reply":"2024-08-20T12:14:39.565967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_psnr(clean, denoised):\n    mse = np.mean((clean - denoised) ** 2)\n    if mse == 0:\n        return 100\n    max_pixel = 1.0\n    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))\n    pnsr = tf.cast(pnsr, tf.float32)\n    return psnr\n\ndef calculate_ssim(clean, denoised):\n    ssim = structural_similarity(clean, denoised, data_range=1.0)\n    ssim = tf.cast(ssim, tf.float32)\n    return ssim","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.567512Z","iopub.status.idle":"2024-08-20T12:14:39.567948Z","shell.execute_reply.started":"2024-08-20T12:14:39.567720Z","shell.execute_reply":"2024-08-20T12:14:39.567738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nimport tensorflow as tf\ntf.keras.mixed_precision.set_global_policy('mixed_float16')\nimport numpy as np\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom sklearn.model_selection import train_test_split\nfrom skimage.metrics import structural_similarity\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Hyperparameters\nnb_epochs = 20\nbatch_size = 8\nlatent_size = 100\nadam_lr = 0.0002\nadam_beta_1 = 0.5\nlambda_l1 = 100  # Weight for L1 loss\n\n# Load and preprocess data\nW = np.load('/kaggle/input/dataset-noise/IRIS_dataset_waveforms_train(1).npy')\nY = np.load('/kaggle/input/dataset-noise/IRIS_dataset_labels_train(1).npy')\n\n# Reshape W to (samples, 3, 12000, 1)\nW = W.reshape(-1, 3, 12000, 1)\n\n# Convert Y to the same shape as W for clean waveforms\nY = Y.reshape(-1, 1, 1, 1)\nY = np.tile(Y, (1, 3, 12000, 1))\n\n# Convert to float32\nW = W.astype('float32')\nY = Y.astype('float32')\n\n# Normalize the data to [-1, 1] range\nW = (W - W.min()) / (W.max() - W.min()) * 2 - 1\nY = (Y - Y.min()) / (Y.max() - Y.min()) * 2 - 1\n\n# Split data\nW_train, W_test, Y_train, Y_test = train_test_split(W, Y, test_size=0.2, random_state=42)\nW_train, W_val, Y_train, Y_val = train_test_split(W_train, Y_train, test_size=0.2, random_state=42)\n\n# Build models (assuming you've defined build_generator and build_discriminator functions)\n\ngenerator = build_generator(latent_size)\ntest_noise = tf.random.normal([1, latent_size])\ntest_input = tf.random.normal([1, 3, 12000, 1])\ntest_output = generator([test_noise, test_input])\nprint(\"Generator output shape:\", test_output.shape)\n\n# Print model summary\ngenerator.summary()\n\n# Check intermediate layer shapes\nfor layer in generator.layers:\n    if isinstance(layer, tf.keras.layers.Conv2D) or isinstance(layer, tf.keras.layers.Conv2DTranspose):\n        print(f\"Layer {layer.name} output shape: {layer.output_shape}\")\n\ndiscriminator = build_discriminator()\n\n# Optimizers with gradient clipping\ngenerator_optimizer = Adam(learning_rate=adam_lr, beta_1=adam_beta_1, clipvalue=1.0)\ndiscriminator_optimizer = Adam(learning_rate=adam_lr, beta_1=adam_beta_1, clipvalue=1.0)\n\n# Loss function\nbinary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(fake_output, denoised_waveforms, clean_waveforms):\n    gan_loss = binary_cross_entropy(tf.ones_like(fake_output), fake_output)\n    denoised_waveforms = tf.cast(denoised_waveforms, tf.float32)\n    clean_waveforms = tf.cast(clean_waveforms, tf.float32)\n    l1_loss = tf.reduce_mean(tf.abs(clean_waveforms - denoised_waveforms))\n    total_loss = gan_loss + lambda_l1 * l1_loss\n    tf.debugging.check_numerics(total_loss, \"Generator loss is not finite\")\n    return gan_loss + lambda_l1 * l1_loss\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = binary_cross_entropy(tf.ones_like(real_output) * 0.9, real_output)\n    fake_loss = binary_cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    tf.debugging.check_numerics(total_loss, \"Discriminator loss is not finite\")\n    return real_loss + fake_loss\n\n# Add L1 loss for direct signal comparison\ndef l1_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.abs(y_true - y_pred))\n\n#@tf.function\ndef train_step(noisy_waveforms, clean_waveforms):\n    logger.info(\"Entering train_step\")\n    noisy_waveforms = tf.cast(noisy_waveforms, tf.float32)\n    clean_waveforms = tf.cast(clean_waveforms, tf.float32)\n    batch_size = tf.shape(noisy_waveforms)[0]\n    noise = tf.random.normal([batch_size, latent_size])\n    \n    logger.info(\"Starting gradient tape\")\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        logger.info(\"Generating waveforms\")\n        generated_waveforms = generator([noise, noisy_waveforms], training=True)\n        \n        '''\n        '''\n        tf.print(\"Noisy waveforms shape:\", tf.shape(noisy_waveforms))\n        tf.print(\"Generated waveforms shape:\", tf.shape(generated_waveforms))\n        tf.print(\"Clean waveforms shape:\", tf.shape(clean_waveforms))\n        '''\n        '''\n        tf.debugging.assert_equal(tf.shape(generated_waveforms), tf.shape(clean_waveforms),\n                                  message=\"Shape mismatch between generated and clean waveforms\")\n        \n        logger.info(\"Running discriminator on clean waveforms\")\n        real_output = discriminator(clean_waveforms, training=True)\n        logger.info(\"Running discriminator on generated waveforms\")\n        fake_output = discriminator(generated_waveforms, training=True)\n        \n        logger.info(\"Calculating losses\")\n        gen_loss = generator_loss(fake_output, generated_waveforms, clean_waveforms)\n        disc_loss = discriminator_loss(real_output, fake_output)\n    \n    logger.info(\"Calculating gradients\")\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    if any(g is None for g in gradients_of_generator):\n        logger.warning(\"Some generator gradients are None\")\n    if any(g is None for g in gradients_of_discriminator):\n        logger.warning(\"Some discriminator gradients are None\")\n        \n    logger.info(\"Applying gradients\")\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    logger.info(\"Exiting train_step\")\n    return gen_loss, disc_loss\n\ndef validation_step(X_val, y_val):\n    X_val = tf.cast(X_val, tf.float32)\n    y_val = tf.cast(y_val, tf.float32)\n    noise = tf.random.normal([len(X_val), latent_size])\n    generated_waveforms = generator([noise, X_val], training=False)\n    \n    real_output = discriminator(y_val, training=False)\n    fake_output = discriminator(generated_waveforms, training=False)\n    \n    gen_loss = generator_loss(fake_output, generated_waveforms, y_val)\n    disc_loss = discriminator_loss(real_output, fake_output)\n    \n    # Calculate PSNR and SSIM\n    psnr = tf.reduce_mean(tf.image.psnr(y_val, generated_waveforms, max_val=1.0))\n    ssim = tf.reduce_mean(tf.image.ssim(y_val, generated_waveforms, max_val=1.0))\n    \n    return disc_loss, gen_loss, psnr, ssim\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((W_train, Y_train)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\nval_dataset = tf.data.Dataset.from_tensor_slices((W_val, Y_val)).batch(batch_size)\ntotal_batches = tf.data.experimental.cardinality(train_dataset).numpy()\nlogger.info(f'Total number of batches: {total_batches}')\n\ntrain_history = {'generator': [], 'discriminator': []}\nval_history = {'generator': [], 'discriminator': [], 'psnr': [], 'ssim': []}\n\nbest_val_loss = float('inf')\nbest_epoch = 0\n\nprint(W.shape, Y.shape)\n\n\n# Training loop\nprev_gen_loss = None\nprev_disc_loss = None\nfor epoch in range(nb_epochs):\n    logger.info(f'Epoch {epoch + 1} of {nb_epochs}')\n    \n    # Training\n    epoch_gen_loss = []\n    epoch_disc_loss = []\n    for i, (noisy_batch, clean_batch) in enumerate(train_dataset):\n        \n        logger.info(f'  Batch {i+1}')\n        try:\n            gen_loss, disc_loss = train_step(noisy_batch, clean_batch)\n            if prev_gen_loss is not None and prev_disc_loss is not None:\n                if tf.math.abs(gen_loss - prev_gen_loss) < 1e-6 and tf.math.abs(disc_loss - prev_disc_loss) < 1e-6:\n                    logger.warning('Loss values are not changing significantly')\n            prev_gen_loss = gen_loss\n            prev_disc_loss = disc_loss\n            epoch_gen_loss.append(gen_loss)\n            epoch_disc_loss.append(disc_loss)\n            logger.info(f'    Gen Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}')\n        except Exception as e:\n            logger.error(f\"Error in batch {i+1}: {str(e)}\")\n            import traceback\n            logger.error(traceback.format_exc())\n            break\n    \n    train_history['discriminator'].append(tf.reduce_mean(epoch_disc_loss))\n    train_history['generator'].append(tf.reduce_mean(epoch_gen_loss))\n    \n    # Validation\n    logger.info(f'Validation for epoch {epoch + 1}:')\n    val_disc_loss = []\n    val_gen_loss = []\n    val_psnr = []\n    val_ssim = []\n    for noisy_batch, clean_batch in val_dataset:\n        disc_loss, gen_loss, psnr, ssim = validation_step(noisy_batch, clean_batch)\n        val_disc_loss.append(disc_loss)\n        val_gen_loss.append(gen_loss)\n        val_psnr.append(psnr)\n        val_ssim.append(ssim)\n    \n    val_history['discriminator'].append(tf.reduce_mean(val_disc_loss))\n    val_history['generator'].append(tf.reduce_mean(val_gen_loss))\n    val_history['psnr'].append(tf.reduce_mean(val_psnr))\n    val_history['ssim'].append(tf.reduce_mean(val_ssim))\n    \n    logger.info(f\"Validation Discriminator Loss: {val_history['discriminator'][-1]}\")\n    logger.info(f\"Validation Generator Loss: {val_history['generator'][-1]}\")\n    logger.info(f\"Validation PSNR: {val_history['psnr'][-1]}\")\n    logger.info(f\"Validation SSIM: {val_history['ssim'][-1]}\")\n    \n    # Check if this is the best model so far\n    current_val_loss = val_history['discriminator'][-1] + val_history['generator'][-1]\n    if current_val_loss < best_val_loss:\n        best_val_loss = current_val_loss\n        best_epoch = epoch\n        # Save the best model\n        generator.save('/path/to/best_generator.h5')\n        discriminator.save('/path/to/best_discriminator.h5')\n    \n    logger.info(f\"Epoch {epoch+1}/{nb_epochs} completed. Current best epoch: {best_epoch+1}\")\n\nlogger.info(f\"Training completed. Best epoch was {best_epoch+1}\")\n\n# Final test on the best model\ngenerator = tf.keras.models.load_model('/path/to/best_generator.h5')\ndiscriminator = tf.keras.models.load_model('/path/to/best_discriminator.h5')\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((W_test, Y_test)).batch(batch_size)\ntest_disc_loss = []\ntest_gen_loss = []\ntest_psnr = []\ntest_ssim = []\n\nfor noisy_batch, clean_batch in test_dataset:\n    disc_loss, gen_loss, psnr, ssim = validation_step(noisy_batch, clean_batch)\n    test_disc_loss.append(disc_loss)\n    test_gen_loss.append(gen_loss)\n    test_psnr.append(psnr)\n    test_ssim.append(ssim)\n\nlogger.info(f\"Final Test Discriminator Loss: {tf.reduce_mean(test_disc_loss)}\")\nlogger.info(f\"Final Test Generator Loss: {tf.reduce_mean(test_gen_loss)}\")\nlogger.info(f\"Final Test PSNR: {tf.reduce_mean(test_psnr)}\")\nlogger.info(f\"Final Test SSIM: {tf.reduce_mean(test_ssim)}\")\n\n# Save the training history for later analysis\nnp.save('/path/to/train_history.npy', train_history)\nnp.save('/path/to/val_history.npy', val_history)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.569349Z","iopub.status.idle":"2024-08-20T12:14:39.569659Z","shell.execute_reply.started":"2024-08-20T12:14:39.569506Z","shell.execute_reply":"2024-08-20T12:14:39.569519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport logging\nimport matplotlib.pyplot as plt\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\nfrom tensorflow.keras.constraints import Constraint\nimport tensorflow.keras.losses as losses\nfrom tensorflow_addons.layers import SpectralNormalization\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Hyperparameters\nnb_epochs = 20\nbatch_size = 32\nlatent_size = 100\nlatent_dim = 100\nadam_lr = 0.0002\nadam_beta_1 = 0.5\nlambda_l1 = 100  # Weight for L1 loss\n\n# Loss functions\nbinary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n'''\ndef generator_loss(fake_output, denoised_waveforms, clean_waveforms):\n    fake_output = tf.cast(fake_output, tf.float32)\n    gan_loss = binary_cross_entropy(tf.ones_like(fake_output), fake_output)\n    denoised_waveforms = tf.cast(denoised_waveforms, tf.float32)\n    clean_waveforms = tf.cast(clean_waveforms, tf.float32)\n    l1_loss = tf.reduce_mean(tf.abs(clean_waveforms - denoised_waveforms))\n    total_loss = gan_loss + lambda_l1 * l1_loss\n    return total_loss, gan_loss, l1_loss\n'''\ndef discriminator_loss(real_output, fake_output):\n    real_loss = binary_cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = binary_cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\n\n# Define loss functions\nbce_loss = losses.BinaryCrossentropy(from_logits=False)\nmae_loss = losses.MeanAbsoluteError()\n\ndef generator_loss(validity, generated_waveforms, clean_waveforms):\n    gan_loss = bce_loss(tf.ones_like(validity), validity)\n    l1_loss = mae_loss(clean_waveforms, generated_waveforms)\n    return gan_loss + lambda_l1 * l1_loss\n'''\n# Training step\n@tf.function\ndef train_step(noisy_waveforms, clean_waveforms, noise):\n    print(\"Entering train_step\")\n    noisy_waveforms = tf.cast(noisy_waveforms, tf.float32)\n    clean_waveforms = tf.cast(clean_waveforms, tf.float32)\n    \n    # Generate noise for the generator input\n    batch_size = tf.shape(noisy_waveforms)[0]\n    #noise = tf.random.normal([batch_size, 100])  # Assuming latent_dim=100\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generate denoised waveforms\n        generated_waveforms = generator([noisy_waveforms, noise], training=True)\n        \n        # Discriminator predictions\n        real_output = discriminator(clean_waveforms, training=True)\n        fake_output = discriminator(generated_waveforms, training=True)\n        \n        # Calculate losses\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(fake_output, generated_waveforms, clean_waveforms)\n        disc_loss = discriminator_loss(real_output, fake_output)\n    \n    # Calculate gradients\n    gen_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    # Clip gradients\n    gen_gradients, _ = tf.clip_by_global_norm(gen_gradients, 1.0)\n    disc_gradients, _ = tf.clip_by_global_norm(disc_gradients, 1.0)\n    \n    # Apply gradients\n    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n    \n    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss\n'''\n\n@tf.function\ndef train_step(noisy_waveforms, clean_waveforms, noise):\n    noisy_waveforms = tf.cast(noisy_waveforms, tf.float32)\n    clean_waveforms = tf.cast(clean_waveforms, tf.float32)\n    \n    # Train discriminator\n    with tf.GradientTape() as disc_tape:\n        generated_waveforms = generator([noisy_waveforms, noise], training=True)\n        real_output = discriminator(clean_waveforms, training=True)\n        fake_output = discriminator(generated_waveforms, training=True)\n        disc_loss = discriminator_loss(real_output, fake_output)\n    \n    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n    \n    # Train generator (using combined model)\n    with tf.GradientTape() as gen_tape:\n        validity, generated_waveforms = combined_model([noisy_waveforms, noise], training=True)\n        gen_loss = generator_loss(validity, generated_waveforms, clean_waveforms)\n    \n    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    \n    return gen_loss, disc_loss\n\n\ndef cal_mse(y_true, y_pred):\n    return tf.reduce_mean(tf.square(y_true - y_pred))\n\ndef cal_snr(y_true, y_pred):\n    noise = y_true - y_pred\n    return 10.0 * tf.math.log(tf.reduce_sum(tf.square(y_true)) / tf.reduce_sum(tf.square(noise))) / tf.math.log(10.0)\n\ndef validation_step(noisy_batch, clean_batch, noise):\n    # Cast inputs to float32\n    noisy_batch = tf.cast(noisy_batch, tf.float32)\n    clean_batch = tf.cast(clean_batch, tf.float32)\n    \n    # Generate noise for the generator input\n    batch_size = tf.shape(noisy_batch)[0]\n    #noise = tf.random.normal([batch_size, 100])  # Assuming latent_dim=100\n    \n    generated_waveforms = generator([noisy_batch, noise], training=False)\n    \n    # Ensure generated_waveforms is float32\n    generated_waveforms = tf.cast(generated_waveforms, tf.float32)\n    \n    real_output = discriminator(clean_batch, training=False)\n    fake_output = discriminator(generated_waveforms, training=False)\n    \n    gen_total_loss = generator_loss(fake_output, generated_waveforms, clean_batch)\n    disc_loss = discriminator_loss(real_output, fake_output)\n    \n    # Calculate MSE and SNR\n    mse_value = cal_mse(clean_batch, generated_waveforms)\n    snr_value = cal_snr(clean_batch, generated_waveforms)\n    return gen_total_loss, disc_loss, mse_value, snr_value\n\n# Build models (assuming you've defined build_generator and build_discriminator functions)\ngenerator = build_generator()\ndiscriminator = build_discriminator()\n\ndef build_combined_model(generator, discriminator):\n    discriminator.trainable = False\n    noise_input = tf.keras.layers.Input(shape=(latent_dim,))\n    waveform_input = tf.keras.layers.Input(shape=(3, 12000, 1))\n    generated_waveform = generator([waveform_input, noise_input])\n    validity = discriminator(generated_waveform)\n    combined = tf.keras.Model(inputs=[waveform_input, noise_input], outputs=[validity, generated_waveform])\n    return combined\n\n# Optimizers\nlr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=adam_lr,\n    decay_steps=1000,\n    decay_rate=0.96\n)\ngenerator_optimizer = Adam(learning_rate=lr_scheduler, beta_1=adam_beta_1)\ndiscriminator_optimizer = Adam(learning_rate=lr_scheduler, beta_1=adam_beta_1)\n\ncombined_model = build_combined_model(generator, discriminator)\ncombined_model.compile(loss=generator_loss, optimizer=generator_optimizer)\n\n\n# Prepare datasets\ntrain_dataset = tf.data.Dataset.from_tensor_slices((W_train, Y_train)).shuffle(1000).batch(batch_size)\nval_dataset = tf.data.Dataset.from_tensor_slices((W_val, Y_val)).batch(batch_size)\ntest_dataset = tf.data.Dataset.from_tensor_slices((W_test, Y_test)).batch(batch_size)\n\n# Training history\ntrain_history = {'generator': [], 'discriminator': []}\nval_history = {'generator': [], 'discriminator': [], 'mse': [], 'snr': []}\n\n# Best model tracking\nbest_val_loss = float('inf')\nbest_epoch = 0\n\npatience = 5\nwait = 0\nbest_val_loss = float('inf')\n\n# Training loop\nfor epoch in range(nb_epochs):\n    print(f'Epoch {epoch + 1} of {nb_epochs}')\n    \n    # Training\n    epoch_gen_loss = []\n    epoch_disc_loss = []\n    for i, (noisy_batch, clean_batch) in enumerate(train_dataset):\n        print(f'  Batch {i+1}')\n        try:\n            noise = tf.random.normal([tf.shape(noisy_batch)[0], 100])\n            gen_loss, disc_loss = train_step(noisy_batch, clean_batch, noise)\n            \n            epoch_gen_loss.append(gen_loss)\n            epoch_disc_loss.append(disc_loss)\n            print(f'Gen Total Loss: {gen_loss.numpy()}, Disc Loss: {disc_loss.numpy()}')\n        except Exception as e:\n            print(f'Error in batch {i+1}: {str(e)}')\n            import traceback\n            logger.error(traceback.format_exc())\n            break\n    \n    train_history['generator'].append(tf.reduce_mean(epoch_gen_loss))\n    train_history['discriminator'].append(tf.reduce_mean(epoch_disc_loss))\n    \n    # Validation\n    print(f'Validation for epoch {epoch + 1}:')\n    val_gen_loss = []\n    val_disc_loss = []\n    val_mse = []\n    val_snr = []\n    for noisy_batch, clean_batch in val_dataset:\n        noise = tf.random.normal([tf.shape(noisy_batch)[0], 100])\n        gen_loss, disc_loss, mse_value, snr_value = validation_step(noisy_batch, clean_batch, noise)\n        val_gen_loss.append(gen_loss)\n        val_disc_loss.append(disc_loss)\n        val_mse.append(mse_value)\n        val_snr.append(snr_value)\n    \n    print(f\"Validation Generator Loss: {tf.reduce_mean(val_gen_loss)}\")\n    print(f\"Validation Discriminator Loss: {tf.reduce_mean(val_disc_loss)}\")\n    print(f\"Validation MSE: {tf.reduce_mean(val_mse)}\")\n    print(f\"Validation SNR: {tf.reduce_mean(val_snr)}\")\n    \n    val_history['generator'].append(tf.reduce_mean(val_gen_loss))\n    val_history['discriminator'].append(tf.reduce_mean(val_disc_loss))\n    val_history['mse'].append(tf.reduce_mean(val_mse))\n    val_history['snr'].append(tf.reduce_mean(val_snr))\n    \n    # Check if this is the best model so far\n    current_val_loss = val_history['generator'][-1] + val_history['discriminator'][-1]\n    if current_val_loss < best_val_loss:\n        best_val_loss = current_val_loss\n        wait = 0\n    else:\n        wait += 1\n        if wait >= patience:\n            print(f\"Early stopping triggered at epoch {epoch+1}\")\n            break\n        best_epoch = epoch\n        # Save the best model\n        generator.save('/kaggle/working/best_generator.h5')\n        discriminator.save('/kaggle/working/best_discriminator.h5')\n        \n    # Inside your training loop, after validation and before the next epoch\n    if epoch % 1 == 0:  # You can adjust this to visualize less frequently if needed\n        # Select a single noisy sample from your validation set\n        noisy_sample = next(iter(val_dataset))[0][0]\n        noisy_sample = tf.expand_dims(noisy_sample, 0)  # Add batch dimension\n\n        # Generate noise\n        noise = tf.random.normal([1, latent_dim])\n\n        # Generate denoised sample\n        generated_sample = generator([noisy_sample, noise], training=False)\n\n        # Plot the results\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        axes[0].imshow(tf.squeeze(noisy_sample).numpy(), aspect='auto', cmap='gray')\n        axes[0].set_title('Noisy Input')\n        axes[0].axis('off')\n\n        axes[1].imshow(tf.squeeze(generated_sample).numpy(), aspect='auto', cmap='gray')\n        axes[1].set_title(f'Denoised Output (Epoch {epoch + 1})')\n        axes[1].axis('off')\n\n        plt.tight_layout()\n        plt.savefig(f'denoising_progress_epoch_{epoch + 1}.png')\n        plt.close()\n\n    print(f'Progress image saved.')\n    print(f\"Epoch {epoch+1}/{nb_epochs} completed. Current best epoch: {best_epoch+1}\")\n    \n    \nprint(f\"Training completed. Best epoch was {best_epoch+1}\")\n\n# Final test on the best model\ncustom_objects = {'SpectralNormalizationConstraint': SpectralNormalizationConstraint}\n\ngenerator = tf.keras.models.load_model('/kaggle/working/best_generator.h5', custom_objects=custom_objects)\ndiscriminator = tf.keras.models.load_model('/kaggle/working/best_discriminator.h5', custom_objects=custom_objects)\n\ntest_gen_loss = []\ntest_disc_loss = []\ntest_mse = []\ntest_snr = []\n\nfor noisy_batch, clean_batch in test_dataset:\n    noise = tf.random.normal([tf.shape(noisy_batch)[0], 100])\n    gen_loss, disc_loss, mse, snr = validation_step(noisy_batch, clean_batch, noise)\n    test_gen_loss.append(gen_loss)\n    test_disc_loss.append(disc_loss)\n    test_mse.append(mse)\n    test_snr.append(snr)\n\nprint(f\"Final Test Generator Loss: {tf.reduce_mean(test_gen_loss)}\")\nprint(f\"Final Test Discriminator Loss: {tf.reduce_mean(test_disc_loss)}\")\nprint(f\"Final Test PSNR: {tf.reduce_mean(test_mse)}\")\nprint(f\"Final Test SSIM: {tf.reduce_mean(test_snr)}\")\n\n# Save the training history for later analysis\nnp.save('/kaggle/working/train_history.npy', train_history)\nnp.save('/kaggle/working/val_history.npy', val_history)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.572986Z","iopub.status.idle":"2024-08-20T12:14:39.573363Z","shell.execute_reply.started":"2024-08-20T12:14:39.573191Z","shell.execute_reply":"2024-08-20T12:14:39.573204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\n# Create plotting lists\nepochs = range(1, nb_epochs + 1)\ntrain_gen_losses = [loss.numpy() for loss in train_history['generator']]\ntrain_disc_losses = [loss.numpy() for loss in train_history['discriminator']]\nval_gen_losses = [loss.numpy() for loss in val_history['generator']]\nval_disc_losses = [loss.numpy() for loss in val_history['discriminator']]\nval_mses = [mse.numpy() for mse in val_history['mse']]\nval_snrs = [snr.numpy() for snr in val_history['snr']]\n\n# Load best weights\ngenerator.load_weights('/kaggle/working/best_generator.h5')\ndiscriminator.load_weights('/kaggle/working/best_discriminator.h5')\n\n# Function to generate and plot images\ndef generate_and_plot_images(model, test_input, test_output, num_examples=6, latent_dim=100):\n    \n    batch_size = test_input.shape[0]\n    noise = tf.random.normal([batch_size, latent_dim])\n    \n    predictions = model([test_input, noise], training=False)\n    \n    fig = plt.figure(figsize=(18, 5*num_examples))\n    \n    for i in range(num_examples):\n        # Plot noisy input\n        ax = fig.add_subplot(num_examples, 3, i*3 + 1)\n        plt.plot(test_input[i, :, 0, 0])\n        plt.title('Noisy')\n        plt.axis('off')\n        \n        # Plot generated output\n        ax = fig.add_subplot(num_examples, 3, i*3 + 2)\n        plt.plot(predictions[i, :, 0, 0])\n        plt.title('Generated')\n        plt.axis('off')\n        \n        # Plot ground truth\n        ax = fig.add_subplot(num_examples, 3, i*3 + 3)\n        plt.plot(test_output[i, :, 0, 0])\n        plt.title('Ground Truth')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('/kaggle/working/generated_images.png')\n    plt.show()\n    \n    return predictions\n    \n# Generate images\ntest_samples = next(iter(test_dataset))\npredictions = generate_and_plot_images(generator, test_samples[0], test_samples[1])\n\n# After generation\nprint(\"Output shape:\", predictions.shape)\nprint(\"Output dtype:\", predictions.dtype)\nprint(\"Output range:\", tf.reduce_min(predictions).numpy(), tf.reduce_max(predictions).numpy())\n\n\n# Final testing\ntest_gen_loss = []\ntest_disc_loss = []\ntest_mse = []\ntest_snr = []\n\nfor noisy_batch, clean_batch in test_dataset:\n    batch_size = noisy_batch.shape[0]\n    latent_dim=100\n    noise = tf.random.normal([batch_size, latent_dim])\n    generated_waveforms = generator([noisy_batch, noise], training=False)\n    \n    real_output = discriminator(clean_batch, training=False)\n    fake_output = discriminator(generated_waveforms, training=False)\n    \n    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(fake_output, generated_waveforms, clean_batch)\n    disc_loss = discriminator_loss(real_output, fake_output)\n    \n    gen_loss, disc_loss, mse_value, snr_value = validation_step(noisy_batch, clean_batch, noise)\n    \n    test_gen_loss.append(gen_total_loss)\n    test_disc_loss.append(disc_loss)\n    test_mse.append(mse_value)\n    test_snr.append(snr_value)\n\n# Check a few values\nprint(\"Sample input values:\", test_samples[0][0, :5, 0, 0])\nprint(\"Sample output values:\", predictions[0, :5, 0, 0])\nprint(\"Sample ground truth values:\", test_samples[1][0, :5, 0, 0])    \n    \n# Print final test results\nprint(f\"Final Test Generator Loss: {tf.reduce_mean(test_gen_loss)}\")\nprint(f\"Final Test Discriminator Loss: {tf.reduce_mean(test_disc_loss)}\")\nprint(f\"Final Test MSE: {tf.reduce_mean(test_mse)}\")\nprint(f\"Final Test SNR: {tf.reduce_mean(test_snr)}\")\n\n# Plot training and validation losses\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_gen_losses, label='Train Generator')\nplt.plot(epochs, val_gen_losses, label='Val Generator')\nplt.title('Generator Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_disc_losses, label='Train Discriminator')\nplt.plot(epochs, val_disc_losses, label='Val Discriminator')\nplt.title('Discriminator Losses')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/loss_plots.png')\nplt.show()\n\n# Plot MSE and SNR\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, val_mses)\nplt.title('Validation MSE')\nplt.xlabel('Epoch')\nplt.ylabel('MSE')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, val_snrs)\nplt.title('Validation SNR')\nplt.xlabel('Epoch')\nplt.ylabel('SNR')\n\nplt.tight_layout()\nplt.savefig('/kaggle/working/metrics_plots.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.574748Z","iopub.status.idle":"2024-08-20T12:14:39.575204Z","shell.execute_reply.started":"2024-08-20T12:14:39.574955Z","shell.execute_reply":"2024-08-20T12:14:39.574973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_samples[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.576344Z","iopub.status.idle":"2024-08-20T12:14:39.576781Z","shell.execute_reply.started":"2024-08-20T12:14:39.576557Z","shell.execute_reply":"2024-08-20T12:14:39.576575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pywt\nimport numpy as np\n\ndef wavelet_denoise(data, wavelet='db1', level=1):\n    coeffs = pywt.wavedec(data, wavelet, mode='soft')\n    threshold = np.sqrt(2 * np.log(len(data)))\n    coeffs[1:] = (pywt.threshold(i, value=threshold, mode='soft') for i in coeffs[1:])\n    return pywt.waverec(coeffs, wavelet, mode='soft')\n\n# Example usage\ndenoised_waveforms = np.array([wavelet_denoise(w) for w in W_test])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.577885Z","iopub.status.idle":"2024-08-20T12:14:39.578333Z","shell.execute_reply.started":"2024-08-20T12:14:39.578089Z","shell.execute_reply":"2024-08-20T12:14:39.578108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.restoration import estimate_sigma, denoise_nl_means\nimport numpy as np\n\ndef nlm_denoise(data, patch_size=5, patch_distance=3, h=0.8):\n    sigma_est = np.mean(estimate_sigma(data))\n    denoised = denoise_nl_means(data, h=h * sigma_est, fast_mode=True,\n                                patch_size=patch_size, patch_distance=patch_distance)\n    return denoised\n\n# Example usage: Process only the first batch\nbatch_size = 10\ndenoised_waveforms_nlm = []\n\nprint(f\"Starting denoising process for {len(W_test)} waveforms in batches of {batch_size}...\")\n\n# Process only the first batch\nstart_index = 0\nend_index = batch_size\nbatch = W_test[start_index:end_index]\n\nprint(f\"Processing batch 1 / {len(W_test) // batch_size + 1}\")\ndenoised_batch = [nlm_denoise(w) for w in batch]\ndenoised_waveforms_nlm.extend(denoised_batch)\nprint(\"Finished batch 1\")\n\ndenoised_waveforms_nlm = np.array(denoised_waveforms_nlm)\n\nprint(\"Denoising process for the first batch completed.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.579700Z","iopub.status.idle":"2024-08-20T12:14:39.580025Z","shell.execute_reply.started":"2024-08-20T12:14:39.579865Z","shell.execute_reply":"2024-08-20T12:14:39.579878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n# Function to calculate SNR\ndef calculate_snr(signal, noise):\n    signal_power = np.mean(signal**2)\n    noise_power = np.mean(noise**2)\n    snr = 10 * np.log10(signal_power / noise_power)\n    return snr\n\n# Load your test data\nX_test, y_test = W_test, Y_test\n\n# Wavelet Denoising\ndenoised_waveforms_wavelet = np.array([wavelet_denoise(w) for w in X_test])\nmse_wavelet = mean_squared_error(y_test.flatten(), denoised_waveforms_wavelet.flatten())\nsnr_wavelet = calculate_snr(y_test.flatten(), y_test.flatten() - denoised_waveforms_wavelet.flatten())\n\nprint(f\"Wavelet Denoising - MSE: {mse_wavelet}, SNR: {snr_wavelet} dB\")\n\n# NLM Denoising\nbatch = 10\nstart_index = 0\nend_index = batch_size\nbatch = W_test[start_index:end_index]\n\ndenoised_waveforms_nlm = np.array([nlm_denoise(w) for w in batch])\nmse_nlm = mean_squared_error(y_test.flatten(), denoised_waveforms_nlm.flatten())\nsnr_nlm = calculate_snr(y_test.flatten(), y_test.flatten() - denoised_waveforms_nlm.flatten())\n\nprint(f\"NLM Denoising - MSE: {mse_nlm}, SNR: {snr_nlm} dB\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:39.581668Z","iopub.status.idle":"2024-08-20T12:14:39.581996Z","shell.execute_reply.started":"2024-08-20T12:14:39.581832Z","shell.execute_reply":"2024-08-20T12:14:39.581846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}